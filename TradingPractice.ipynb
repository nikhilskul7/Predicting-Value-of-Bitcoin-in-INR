{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TradingPractice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTo9ou0EjcNMfb28YzQ+yf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilskul7/Predicting-Value-of-Bitcoin-in-INR/blob/master/TradingPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pLdYdZnyQpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings;\n",
        "warnings.filterwarnings('ignore');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnCTa_aP0XzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import Request, urlopen\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import seaborn as sn\n",
        "import time\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.preprocessing as prep\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from pandas_datareader import data\n",
        "import datetime as dt\n",
        "import os\n",
        "import tensorflow as tf \n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOZuIpj41DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "APIKey=\"8VOSO8I1MPY69CA5\"\n",
        "cryptocurrencytoconvert=\"BTC\"\n",
        "smoothing_window_size = 100\n",
        "market=\"INR\"\n",
        "EMA = 0.0\n",
        "gamma = 0.1\n",
        "window_size = 100\n",
        "epochstesting=200\n",
        "numDays=4\n",
        "time_step = 1000\n",
        "exmaWindow = 3\n",
        "smaWindowSmall = 5\n",
        "smaWindowLarge = 25\n",
        "emaWindowSmall = 10\n",
        "emaWindowLarge = 25\n",
        "\n",
        "normalisationFactor = 1.5\n",
        "\n",
        "url=\"https://www.alphavantage.co/query?function=DIGITAL_CURRENCY_DAILY&symbol=\"+cryptocurrencytoconvert+\"&market=\"+market+\"&apikey=\"+APIKey\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLuZ8I8PFTZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createHistodata():\n",
        "  request=\"https://www.alphavantage.co/query?function=DIGITAL_CURRENCY_DAILY&symbol=\"+cryptocurrencytoconvert+\"&market=\"+market+\"&apikey=\"+APIKey\n",
        "  response=urlopen(request)\n",
        "  data=json.loads(response.read())\n",
        "  df=pd.DataFrame.from_dict(data['Time Series (Digital Currency Daily)'], orient=\"index\")\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK1IbvsjOL9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanData(df):\n",
        "  df=df.drop(['1b. open (USD)', '2b. high (USD)','3b. low (USD)','4b. close (USD)'], axis=1)\n",
        "  df=df.rename(columns={\"1a. open (INR)\": \"Open\", \"2a. high (INR)\": \"High\", \"3a. low (INR)\": \"Low\", \"4a. close (INR)\": \"Close\", \"5. volume\": \"Volume\", \"6. market cap (USD)\": \"Market Cap\"})\n",
        "  df['Open']=df['Open'].astype(float)\n",
        "  df['High']=df['High'].astype(float)\n",
        "  df['Low']=df['Low'].astype(float)\n",
        "  df['Close']=df['Close'].astype(float)\n",
        "  df['Volume']=df['Volume'].astype(float)\n",
        "  df['Market Cap']=df['Market Cap'].astype(float)\n",
        "  df.dropna(inplace=True)\n",
        "  df.isna().sum()\n",
        "  \n",
        "  df=df.drop([ 'Market Cap'], axis=1)\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pry_iKePFVPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liktIDTtxI9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aan2ELIrzmAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "145b21dc-b1cc-498a-877d-62c7219fc40e"
      },
      "source": [
        "''' plt.figure(figsize=(10,7))\n",
        "sn.heatmap(df.corr(),annot=True,cmap = 'Blues',vmin=-1,vmax=1,center=0,linewidths=2, linecolor='black')\n",
        "plt.xticks(fontsize=15,rotation=90)\n",
        "plt.yticks(fontsize=15,rotation=0)\n",
        "plt.title('Correlation HeatMap')\n",
        "plt.show() '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" plt.figure(figsize=(10,7))\\nsn.heatmap(df.corr(),annot=True,cmap = 'Blues',vmin=-1,vmax=1,center=0,linewidths=2, linecolor='black')\\nplt.xticks(fontsize=15,rotation=90)\\nplt.yticks(fontsize=15,rotation=0)\\nplt.title('Correlation HeatMap')\\nplt.show() \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y2cwiDlWUz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def splitData(df):\n",
        "  train_data = df[:800]\n",
        "  test_data = df[800:]\n",
        "  return train_data,test_data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB6B2nV7baTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removeNullValues(dataframe):\n",
        "  dataframe.isna().sum()\n",
        "  dataframe.dropna(inplace=True)\n",
        "  dataframe.isna().sum()\n",
        "  dataframe.reset_index(drop=True, inplace=True)\n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIDqpZQIbZUu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqn14dIYizFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeData(train_data,test_data,df):\n",
        "  scaler = MinMaxScaler()\n",
        "  EMA=0.0\n",
        "  ''' plt.figure(figsize = (18,9))\n",
        "  plt.plot(range(df.shape[0]),df['Open'],color='b',label='True')\n",
        "      \n",
        "  #plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
        "      \n",
        "  #plt.xticks(range(0,df.shape[0],50),df['Date'],rotation=45)\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Price')\n",
        "  plt.legend(fontsize=18)\n",
        "  plt.show() '''\n",
        "  \n",
        "  for di in range(0,100,smoothing_window_size):\n",
        "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
        "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
        "    scaler.fit(train_data[di+smoothing_window_size:,:])\n",
        "    train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n",
        "    train_data = train_data.reshape(-1)\n",
        "    test_data = scaler.transform(test_data).reshape(-1)\n",
        "    for ti in range(800):\n",
        "        EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
        "        train_data[ti] = EMA\n",
        "    N = train_data.size\n",
        "    std_avg_predictions = []\n",
        "    std_avg_x = []\n",
        "    mse_errors = []\n",
        "  \n",
        "    for pred_idx in range(window_size,N):\n",
        "\n",
        "        if pred_idx >= N:\n",
        "            date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
        "        else:\n",
        "            date = df.loc[pred_idx,'Date']\n",
        "\n",
        "        std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
        "        mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
        "        std_avg_x.append(date)\n",
        "    rms = sqrt(mean_squared_error(y_actual, y_predicted))\n",
        "\n",
        "    print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))\n",
        "    \n",
        "    \n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhFNXGjsfdDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createDataset(dataframe, index):\n",
        "  output_df = pd.DataFrame(dataframe.iloc[:,0])\n",
        "  output_df = output_df.iloc[index:]\n",
        "  output_df.reset_index(drop=True, inplace=True)\n",
        "  lastArray = dataframe.values[-1].tolist()\n",
        "  \n",
        "  dataframe.drop(dataframe.tail(index).index,inplace=True)\n",
        "  return dataframe, output_df,lastArray\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP65IiEQPjFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normaliseInputData(inputdf, maxValue, length):\n",
        "  openArray = inputdf.iloc[:,0].values\n",
        "  print(\"Normalising OPEN prices\", end='\\n'*2)\n",
        "  for i in range(length):\n",
        "    openArray[i] = float(openArray[i]/maxValue)\n",
        "\n",
        "  exmaArray = inputdf.iloc[:,1].values\n",
        "  print(\"Normalising EXMA prices\", end='\\n'*2)\n",
        "  for i in range(length):\n",
        "    exmaArray[i] = float(exmaArray[i]/maxValue)\n",
        "\n",
        "  smaArray = inputdf.iloc[:,2].values\n",
        "  print(\"Normalising SMA prices\", end='\\n'*2)\n",
        "  for i in range(length):\n",
        "    smaArray[i] = float(smaArray[i]/maxValue)\n",
        "\n",
        "  smaLongArray = inputdf.iloc[:,3].values\n",
        "  print(\"Normalising SMA Long prices\", end='\\n'*2)\n",
        "  for i in range(length):\n",
        "    smaLongArray[i] = float(smaLongArray[i]/maxValue)\n",
        "\n",
        "  emaArray = inputdf.iloc[:,4].values\n",
        "  print(\"Normalising EMA prices\", end='\\n'*2)\n",
        "  for i in range(length):\n",
        "    emaArray[i] = float(emaArray[i]/maxValue)\n",
        "\n",
        "  emaLongArray = inputdf.iloc[:,5].values\n",
        "  print(\"Normalising EMA prices\", end='\\n'*2)\n",
        "  for i in range(length):\n",
        "    emaLongArray[i] = float(emaLongArray[i]/maxValue)\n",
        "    \n",
        "  inputArray = []\n",
        "  for i in range(length):\n",
        "    shapeArray = [openArray[i], exmaArray[i], smaArray[i], smaLongArray[i], emaArray[i], emaLongArray[i]]\n",
        "    inputArray.append(shapeArray)\n",
        "  return np.array(inputArray)\n",
        "\n",
        "def normaliseOutputData(inputdf, outputdf, maxValue, length):\n",
        "  outputArray = outputdf.iloc[:,0].values\n",
        "  print(\"Normalising Output array\", end='\\n'*2)\n",
        "  for i in (range(length)):\n",
        "    outputArray[i] = float(outputArray[i]/maxValue)\n",
        "  return np.array(outputArray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h-1MX_UQ7pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculateParameters(simpleDataframe):\n",
        "  # No formatting required so do nothing here\n",
        "  dataframe = simpleDataframe\n",
        "  df_open = pd.DataFrame(dataframe.iloc[:,0])\n",
        "  #print(df_open.head())\n",
        "  dataframe['<EXMA>'] = df_open.expanding(min_periods=exmaWindow).mean()\n",
        "  dataframe['<SMA_SMALL>'] = df_open.rolling(window=smaWindowSmall).mean()\n",
        "  dataframe['<SMA_LARGE>'] = df_open.rolling(window=smaWindowLarge).mean()\n",
        "  dataframe['<EMA_SMALL>'] = df_open.ewm(span=emaWindowSmall,adjust=False).mean()\n",
        "  dataframe['<EMA_LARGE>'] = df_open.ewm(span=emaWindowLarge,adjust=False).mean()\n",
        "  #print(dataframe.head())\n",
        "  return dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Cx0snAc332",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateDataAndTrain(index,model):\n",
        "  df=createHistodata()\n",
        "  df=cleanData(df)\n",
        "\n",
        "  df= calculateParameters(df)\n",
        "\n",
        "  df = removeNullValues(df)\n",
        "  print(df)\n",
        "  xtrain, ytrain,lastColumn = createDataset(df, index)\n",
        "  \n",
        "  lastColumn = xtrain[\"Open\"].tolist()\n",
        "  maxValue = (max(lastColumn)) * normalisationFactor\n",
        "  length = len(lastColumn)\n",
        "  \n",
        "  input = normaliseInputData(xtrain, maxValue, length)\n",
        "  output = normaliseOutputData(xtrain, ytrain, maxValue, length)\n",
        "\n",
        "  trimmedLastArray = []\n",
        "  for i in range(len(lastColumn)):\n",
        "    trimmedLastArray.append(float(lastColumn[i]/maxValue))\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "  model.fit(input, output, validation_split=0.2, epochs=epochstesting, batch_size=16)\n",
        "  prediction = model.predict(np.array([input[-1]]))\n",
        "  predictedValue = float(prediction*maxValue)\n",
        "  return predictedValue\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrL9Q-Lzd4J-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a8c597d-e732-4257-bfe4-3c66d59d32e6"
      },
      "source": [
        "output=[]\n",
        "def trainingModel():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, activation='relu', input_shape=(6,)))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(5, activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  \n",
        "  myPredictions = []\n",
        "  for i in range(numDays):\n",
        "    myPredictions.append(generateDataAndTrain(i, model))\n",
        "  return myPredictions\n",
        "y = trainingModel()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             Open          High  ...    <EMA_SMALL>    <EMA_LARGE>\n",
            "0    692025.16086  700342.33500  ...  699645.238030  736397.403202\n",
            "1    696096.06462  696915.94104  ...  698999.933774  733297.300234\n",
            "2    691905.25206  698279.90364  ...  697709.991644  730113.296528\n",
            "3    707166.64458  707521.12497  ...  699429.383087  728348.169455\n",
            "4    693777.32820  709710.21000  ...  698401.736744  725688.873974\n",
            "..            ...           ...  ...            ...            ...\n",
            "971  484580.68857  500992.45614  ...  569777.341558  702541.546058\n",
            "972  437662.62342  501928.49421  ...  545756.483715  682166.244317\n",
            "973  468022.78215  496872.09000  ...  531623.083430  665693.670304\n",
            "974  487354.32900  509461.01514  ...  523574.218989  651975.259434\n",
            "975  530521.49700  545578.29513  ...  524837.360445  642632.662324\n",
            "\n",
            "[976 rows x 10 columns]\n",
            "Normalising OPEN prices\n",
            "\n",
            "Normalising EXMA prices\n",
            "\n",
            "Normalising SMA prices\n",
            "\n",
            "Normalising SMA Long prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising Output array\n",
            "\n",
            "Epoch 1/200\n",
            "49/49 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.1100\n",
            "Epoch 2/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0420 - val_loss: 0.0858\n",
            "Epoch 3/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0288 - val_loss: 0.0673\n",
            "Epoch 4/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0532\n",
            "Epoch 5/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0433\n",
            "Epoch 6/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.0360\n",
            "Epoch 7/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.0311\n",
            "Epoch 8/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0069 - val_loss: 0.0279\n",
            "Epoch 9/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0255\n",
            "Epoch 10/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0241\n",
            "Epoch 11/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.0232\n",
            "Epoch 12/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0225\n",
            "Epoch 13/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 14/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 15/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 16/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 17/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 18/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 19/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 20/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 21/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 22/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 23/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 24/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 25/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 26/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 27/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 28/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 29/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 30/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 31/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 32/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 33/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 34/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 35/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 36/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 37/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 38/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 39/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 40/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 41/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 42/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 43/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 44/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 45/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 46/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 47/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 48/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 49/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 50/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 51/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 52/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 53/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 54/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 55/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 56/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 57/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 58/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 59/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 60/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 61/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 62/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 63/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 64/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 65/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 66/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 67/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 68/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 69/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 70/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 71/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 72/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 73/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 74/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 75/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 76/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 77/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 78/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 79/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 80/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 81/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 82/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 83/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 84/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 85/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 86/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 87/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 88/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 89/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 90/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 91/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 92/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 93/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 94/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 95/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 96/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 97/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 98/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 99/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 100/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 101/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 102/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 103/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 104/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 105/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 106/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 107/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 108/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 109/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 110/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 111/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 112/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 113/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 114/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 115/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 116/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 117/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 118/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 119/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 120/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 121/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0211\n",
            "Epoch 122/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 123/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 124/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 125/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 126/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 127/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 128/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 129/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 130/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 131/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 132/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 133/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 134/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 135/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 136/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 137/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 138/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 139/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 140/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 141/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 142/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 143/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 144/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 145/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 146/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 147/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 148/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 149/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 150/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 151/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 152/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 153/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 154/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 155/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 156/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 157/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 158/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 159/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 160/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 161/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 162/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 163/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 164/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 165/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 166/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 167/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 168/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 169/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 170/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 171/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 172/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 173/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 174/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 175/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0211\n",
            "Epoch 176/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 177/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 178/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 179/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 180/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 181/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 182/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 183/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0210\n",
            "Epoch 184/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 185/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 186/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 187/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 188/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 189/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 190/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 191/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 192/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 193/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 194/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 195/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 196/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 197/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 198/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 199/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 200/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10b7fb9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "             Open          High  ...    <EMA_SMALL>    <EMA_LARGE>\n",
            "0    692025.16086  700342.33500  ...  699645.238030  736397.403202\n",
            "1    696096.06462  696915.94104  ...  698999.933774  733297.300234\n",
            "2    691905.25206  698279.90364  ...  697709.991644  730113.296528\n",
            "3    707166.64458  707521.12497  ...  699429.383087  728348.169455\n",
            "4    693777.32820  709710.21000  ...  698401.736744  725688.873974\n",
            "..            ...           ...  ...            ...            ...\n",
            "971  484580.68857  500992.45614  ...  569777.341558  702541.546058\n",
            "972  437662.62342  501928.49421  ...  545756.483715  682166.244317\n",
            "973  468022.78215  496872.09000  ...  531623.083430  665693.670304\n",
            "974  487354.32900  509461.01514  ...  523574.218989  651975.259434\n",
            "975  530521.49700  545578.29513  ...  524837.360445  642632.662324\n",
            "\n",
            "[976 rows x 10 columns]\n",
            "Normalising OPEN prices\n",
            "\n",
            "Normalising EXMA prices\n",
            "\n",
            "Normalising SMA prices\n",
            "\n",
            "Normalising SMA Long prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising Output array\n",
            "\n",
            "Epoch 1/200\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 2/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 3/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 4/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 5/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 6/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 7/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 8/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 9/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 10/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 11/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 12/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 13/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 14/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 15/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 16/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 17/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 18/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 19/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 20/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 21/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 22/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 23/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 24/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 25/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 26/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 27/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 28/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 29/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 30/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 31/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 32/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 33/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 34/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 35/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 36/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 37/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 38/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 39/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 40/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 41/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 42/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 43/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 44/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 45/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 46/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 47/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 48/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 49/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 50/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 51/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 52/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 53/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 54/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 55/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 56/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 57/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 58/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 59/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 60/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 61/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 62/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 63/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 64/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 65/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 66/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 67/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 68/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 69/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 70/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 71/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 72/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 73/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 74/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 75/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 76/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 77/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 78/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 79/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 80/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 81/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 82/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 83/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 84/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 85/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 86/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 87/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 88/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 89/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 90/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 91/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 92/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 93/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 94/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 95/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 96/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 97/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 98/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 99/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 100/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 101/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 102/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 103/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 104/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 105/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 106/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 107/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 108/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 109/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 110/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 111/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 112/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 113/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 114/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 115/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 116/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 117/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 118/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 119/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 120/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 121/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 122/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 123/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 124/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 125/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 126/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 127/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 128/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 129/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 130/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 131/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 132/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 133/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 134/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0225\n",
            "Epoch 135/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 136/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 137/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 138/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 139/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 140/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 141/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 142/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 143/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 144/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 145/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 146/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 147/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 148/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 149/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 150/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 151/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 152/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 153/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 154/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 155/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 156/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 157/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 158/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 159/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 160/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 161/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 162/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 163/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 164/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 165/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 166/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 167/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 168/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 169/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 170/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 171/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 172/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 173/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 174/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 175/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 176/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 177/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 178/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 179/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 180/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0208\n",
            "Epoch 181/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 182/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 183/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 184/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 185/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 186/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 187/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 188/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 189/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 190/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 191/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 192/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 193/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 194/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 195/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 196/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 197/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 198/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 199/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 200/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10b667f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "             Open          High  ...    <EMA_SMALL>    <EMA_LARGE>\n",
            "0    692025.16086  700342.33500  ...  699645.238030  736397.403202\n",
            "1    696096.06462  696915.94104  ...  698999.933774  733297.300234\n",
            "2    691905.25206  698279.90364  ...  697709.991644  730113.296528\n",
            "3    707166.64458  707521.12497  ...  699429.383087  728348.169455\n",
            "4    693777.32820  709710.21000  ...  698401.736744  725688.873974\n",
            "..            ...           ...  ...            ...            ...\n",
            "971  484580.68857  500992.45614  ...  569777.341558  702541.546058\n",
            "972  437662.62342  501928.49421  ...  545756.483715  682166.244317\n",
            "973  468022.78215  496872.09000  ...  531623.083430  665693.670304\n",
            "974  487354.32900  509461.01514  ...  523574.218989  651975.259434\n",
            "975  530521.49700  545578.29513  ...  524837.360445  642632.662324\n",
            "\n",
            "[976 rows x 10 columns]\n",
            "Normalising OPEN prices\n",
            "\n",
            "Normalising EXMA prices\n",
            "\n",
            "Normalising SMA prices\n",
            "\n",
            "Normalising SMA Long prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising Output array\n",
            "\n",
            "Epoch 1/200\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 2/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 3/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 4/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 5/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 6/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 7/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 8/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 9/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 10/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 11/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 12/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 13/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 14/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 15/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 16/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 17/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 18/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 19/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 20/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 21/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 22/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 23/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 24/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 25/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 26/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 27/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 28/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 29/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0226\n",
            "Epoch 30/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 31/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 32/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 33/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 34/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 35/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 36/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 37/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 38/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 39/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 40/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 41/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 42/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 43/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 44/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 45/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 46/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 47/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 48/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 49/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 50/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 51/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 52/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 53/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 54/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 55/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 56/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 57/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 58/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 59/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 60/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 61/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 62/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 63/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 64/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 65/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 66/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 67/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 68/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 69/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 70/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 71/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 72/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 73/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 74/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 75/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 76/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 77/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 78/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 79/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 80/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 81/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 82/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 83/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 84/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 85/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 86/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 87/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 88/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 89/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 90/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 91/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 92/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 93/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 94/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 95/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 96/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 97/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 98/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 99/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 100/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 101/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 102/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 103/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 104/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 105/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0226\n",
            "Epoch 106/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 107/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 108/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 109/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 110/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 111/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 112/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 113/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 114/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 115/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 116/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 117/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 118/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 119/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 120/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 121/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 122/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 123/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 124/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 125/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 126/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 127/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 128/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 129/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 130/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 131/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 132/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 133/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 134/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 135/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 136/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 137/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 138/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 139/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 140/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 141/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 142/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 143/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 144/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 145/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 146/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0225\n",
            "Epoch 147/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 148/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 149/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 150/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 151/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 152/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 153/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 154/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 155/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 156/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 157/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 158/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 159/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 160/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 161/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 162/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 163/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 164/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 165/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 166/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 167/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 168/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 169/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 170/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 171/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 172/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 173/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 174/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0225\n",
            "Epoch 175/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 176/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 177/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 178/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 179/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 180/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 181/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 182/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 183/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 184/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 185/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0226\n",
            "Epoch 186/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 187/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 188/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 189/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 190/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 191/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 192/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 193/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 194/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 195/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 196/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 197/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 198/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 199/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 200/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10b4d1fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "             Open          High  ...    <EMA_SMALL>    <EMA_LARGE>\n",
            "0    692025.16086  700342.33500  ...  699645.238030  736397.403202\n",
            "1    696096.06462  696915.94104  ...  698999.933774  733297.300234\n",
            "2    691905.25206  698279.90364  ...  697709.991644  730113.296528\n",
            "3    707166.64458  707521.12497  ...  699429.383087  728348.169455\n",
            "4    693777.32820  709710.21000  ...  698401.736744  725688.873974\n",
            "..            ...           ...  ...            ...            ...\n",
            "971  484580.68857  500992.45614  ...  569777.341558  702541.546058\n",
            "972  437662.62342  501928.49421  ...  545756.483715  682166.244317\n",
            "973  468022.78215  496872.09000  ...  531623.083430  665693.670304\n",
            "974  487354.32900  509461.01514  ...  523574.218989  651975.259434\n",
            "975  530521.49700  545578.29513  ...  524837.360445  642632.662324\n",
            "\n",
            "[976 rows x 10 columns]\n",
            "Normalising OPEN prices\n",
            "\n",
            "Normalising EXMA prices\n",
            "\n",
            "Normalising SMA prices\n",
            "\n",
            "Normalising SMA Long prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising EMA prices\n",
            "\n",
            "Normalising Output array\n",
            "\n",
            "Epoch 1/200\n",
            "49/49 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 2/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 3/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 4/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 5/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 6/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 7/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 8/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 9/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 10/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 11/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 12/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 13/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 14/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 15/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 16/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 17/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 18/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 19/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 20/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 21/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 22/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 23/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 24/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 25/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 26/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 27/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 28/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 29/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 30/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 31/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 32/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 33/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 34/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 35/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 36/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 37/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 38/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 39/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 40/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 41/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 42/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 43/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 44/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 45/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 46/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 47/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 48/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 49/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 50/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 51/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 52/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 53/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 54/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 55/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 56/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 57/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 58/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 59/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 60/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 61/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 62/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 63/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 64/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 65/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 66/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 67/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 68/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 69/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 70/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 71/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 72/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 73/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 74/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 75/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 76/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 77/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 78/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 79/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 80/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 81/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 82/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 83/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 84/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 85/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 86/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 87/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 88/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 89/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 90/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0225\n",
            "Epoch 91/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 92/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 93/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 94/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 95/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 96/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 97/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 98/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 99/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 100/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 101/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 102/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 103/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 104/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 105/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 106/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 107/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 108/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 109/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 110/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 111/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 112/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 113/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 114/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 115/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 116/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0224\n",
            "Epoch 117/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 118/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 119/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 120/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 121/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 122/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 123/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 124/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 125/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 126/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 127/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 128/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 129/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 130/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 131/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 132/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 133/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 134/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 135/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 136/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 137/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 138/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 139/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 140/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 141/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 142/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 143/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 144/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 145/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 146/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 147/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 148/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 149/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 150/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 151/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 152/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 153/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0226\n",
            "Epoch 154/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 155/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 156/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0213\n",
            "Epoch 157/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 158/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 159/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 160/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 161/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 162/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 163/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 164/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 165/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 166/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 167/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 168/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 169/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0225\n",
            "Epoch 170/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 171/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 172/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "Epoch 173/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0212\n",
            "Epoch 174/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 175/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0222\n",
            "Epoch 176/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 177/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 178/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0214\n",
            "Epoch 179/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 180/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 181/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0221\n",
            "Epoch 182/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 183/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 184/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 185/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 186/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 187/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0218\n",
            "Epoch 188/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 189/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 190/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 191/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 192/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0215\n",
            "Epoch 193/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 194/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0216\n",
            "Epoch 195/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 196/200\n",
            "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0217\n",
            "Epoch 197/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0219\n",
            "Epoch 198/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 199/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0220\n",
            "Epoch 200/200\n",
            "49/49 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0223\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10b33c5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT2RAaf2KZ7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y)):\n",
        "  y[i]=float(y[i])\n",
        "\n",
        "scaleFactor=(y[0]+max(y))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLqSmjYPeirt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dcc6f0c-cf9d-4975-80a4-8a2d9bdfc0ac"
      },
      "source": [
        "priceUrl = 'https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE&from_currency='+cryptocurrencytoconvert+'&to_currency='+market+'&apikey='+APIKey\n",
        "price = float(requests.get(priceUrl).json()['Realtime Currency Exchange Rate']['5. Exchange Rate'])\n",
        "print(price)\n",
        "scaleFactor = price/scaleFactor\n",
        "\n",
        "for i in range(len(y)):\n",
        "  y[i] = y[i] * scaleFactor\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "842868.8639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtgSp84GctWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "93b92813-7ab4-40e0-d43b-eff75f84531a"
      },
      "source": [
        "x = []\n",
        "for i in range(len(y)):\n",
        "  x.append(float(i))\n",
        "# plotting the points  \n",
        "plt.plot(x, y) \n",
        "  \n",
        "# naming the x axis \n",
        "plt.xlabel('Number of days from today') \n",
        "# naming the y axis \n",
        "plt.ylabel('Exchange Rate') \n",
        "  \n",
        "# giving a title to my graph \n",
        "plt.title('Price Graph') \n",
        "# function to show the plot \n",
        "plt.show() \n",
        "\n",
        "print('Price Data for the bitcoin')\n",
        "for i in range(len(y)):\n",
        "  print(str(int(x[i])) + ' Days after today, The price will be: ' + str(y[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Price Data for the bitcoin\n",
            "0 Days after today, The price will be: 841669.9043320558\n",
            "1 Days after today, The price will be: 844067.8234679442\n",
            "2 Days after today, The price will be: 840704.9839728563\n",
            "3 Days after today, The price will be: 830762.7759474632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXa-AmODyVt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9656dcd3-5e9f-4f72-a043-17f5080ec480"
      },
      "source": [
        "fig = plt.figure()\n",
        "low = min(y)\n",
        "high = max(y)\n",
        "plt.ylim([math.ceil(low-0.5*(high-low)), math.ceil(high+0.5*(high-low))])\n",
        "plt.title(\"Prices against Days\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Prices in Rupees\")\n",
        "plt.bar(x,y) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTLV0xxr0P5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}